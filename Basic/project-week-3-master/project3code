!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark
!pip install pyspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
print(os.listdir("/content/"))

from pyspark.sql import SparkSession
import pyspark.sql as sparksql
spark = SparkSession.builder.appName('stroke').getOrCreate()
train = spark.read.csv('/content/train.csv', inferSchema=True,header=True)
test = spark.read.csv('/content/test.csv', inferSchema=True,header=True)

train.printSchema()
train.dtypes
train.head(5)
train.toPandas().head(5)
train.groupBy('stroke').count().show()

# create DataFrame as a temporary view for SQL queries
train.createOrReplaceTempView('table')

# sql query to find the number of people in specific work_type who have had stroke and not
spark.sql("SELECT work_type, COUNT(work_type) as work_type_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY COUNT(work_type) DESC").show()
spark.sql("SELECT work_type, COUNT(work_type) as work_type_count FROM table WHERE stroke == 0 GROUP BY work_type ORDER BY COUNT(work_type) DESC").show()

spark.sql("SELECT gender, COUNT(gender) as gender_count, COUNT(gender)*100/(SELECT COUNT(gender) FROM table WHERE gender == 'Male') as percentage FROM table WHERE stroke== 1 AND gender = 'Male' GROUP BY gender").show()
spark.sql("SELECT gender, COUNT(gender) as gender_count, COUNT(gender)*100/(SELECT COUNT(gender) FROM table WHERE gender == 'Female') as percentage FROM table WHERE stroke== 1 AND gender = 'Female' GROUP BY gender").show()

spark.sql("SELECT COUNT(age)*100/(SELECT COUNT(age) FROM table WHERE stroke ==1) as percentage FROM table WHERE stroke == 1 AND age>=50").show()

train.describe().show()
test.describe().show()

# fill in missing values for smoking status
train_f = train.na.fill({'smoking_status': 'idk', 'heart_disease': 2, 'ever_married': 'idk', 'work_type': 'idk', 'Residence_type': 'idk', 'stroke': 2})
test_f = test.na.fill({'smoking_status': 'idk', 'Residence_type': 'idk'})

# fill in miss values for bmi 
# as this is numecial data , we will simple fill the missing values with mean
from pyspark.sql.functions import mean
meano = train_f.select(mean(train_f['bmi'])).collect()
mean_bmi = meano[0][0]
train_f = train_f.na.fill(mean_bmi,['bmi'])
test_f = test_f.na.fill(mean_bmi,['bmi'])

meann = train_f.select(mean(train_f['avg_glucose_level'])).collect()
mean_avg_glucose_level = meann[0][0]
train_f = train_f.na.fill(mean_avg_glucose_level,['avg_glucose_level'])
test_f = test_f.na.fill(mean_avg_glucose_level,['avg_glucose_level'])



train_f.describe().show()
test_f.describe().show()

# indexing all categorical columns in the dataset
from pyspark.ml.feature import StringIndexer
indexer1 = StringIndexer(inputCol="gender", outputCol="genderIndex")
indexer2 = StringIndexer(inputCol="ever_married", outputCol="ever_marriedIndex")
indexer3 = StringIndexer(inputCol="work_type", outputCol="work_typeIndex")
indexer4 = StringIndexer(inputCol="Residence_type", outputCol="Residence_typeIndex")
indexer5 = StringIndexer(inputCol="smoking_status", outputCol="smoking_statusIndex")

# Doing one hot encoding of indexed data
from pyspark.ml.feature import OneHotEncoderEstimator
encoder = OneHotEncoderEstimator(inputCols=["genderIndex","ever_marriedIndex","work_typeIndex","Residence_typeIndex","smoking_statusIndex"],
                                 outputCols=["genderVec","ever_marriedVec","work_typeVec","Residence_typeVec","smoking_statusVec"])
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=['genderVec',
 'age',
 'hypertension',
 'heart_disease',
 'ever_marriedVec',
 'work_typeVec',
 'Residence_typeVec',
 'avg_glucose_level',
 'bmi',
 'smoking_statusVec'],outputCol='features')

#train_f.describe().show()
#test_f.describe().show()


# DECISION TREE CLASSIFIER
from pyspark.ml.classification import DecisionTreeClassifier
dtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')

from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[indexer1, indexer2, indexer3, indexer4, indexer5, encoder, assembler, dtc])
# splitting training and validation data
train_data,val_data = train_f.randomSplit([0.7,0.3])
# training model pipeline with data
model = pipeline.fit(train_data)
# making prediction on model with validation data
dtc_predictions = model.transform(val_data)
# Select example rows to display.
#dtc_predictions.select("prediction","probability", "stroke", "features").show(5)
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
# Select (prediction, true label) and compute test error
acc_evaluator = MulticlassClassificationEvaluator(labelCol="stroke", predictionCol="prediction", metricName="accuracy")
dtc_acc = acc_evaluator.evaluate(dtc_predictions)
print('A Decision Tree algorithm had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))
# now predicting the labels for test data
test_pred = model.transform(test_f)
print("DECISION TREE PREDICTIONS")
test_selected = test_pred.select("id", "features", "prediction","probability")
test_selected.limit(5).toPandas()

# LOGISTIC REGRESSION
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(labelCol='stroke',featuresCol='features', maxIter=10)

lr_train_data,lr_val_data = train_f.randomSplit([0.7,0.3])
lrpipeline = Pipeline(stages=[indexer1, indexer2, indexer3, indexer4, indexer5, encoder, assembler, lr])
lrModel = lrpipeline.fit(lr_train_data)

import matplotlib.pyplot as plt
import numpy as np

lrpredictions = lrModel.transform(lr_val_data)
#lrpredictions.select("prediction","probability", "stroke", "features").show(5)
#lrtestpredictions = lrModel.transform(test_f)
#lrtestpredictions.select("prediction","probability", "stroke", "features").show(5)
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(labelCol="stroke")
print('Logistic Regression Test Area Under ROC: ', evaluator.evaluate(lrpredictions))
# now predicting the labels for test data
test_pred_lr = lrModel.transform(test_f)
print("LR PREDICTIONS")
test_selected_lr = test_pred_lr.select("id", "features", "prediction","probability")
test_selected_lr.limit(5).toPandas()

# CLUSTERING ...

from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

kmeans = KMeans(k=2,seed=1)
cpipeline = Pipeline(stages=[indexer1, indexer2, indexer3, indexer4, indexer5, encoder, assembler, kmeans])
cModel = cpipeline.fit(test_f)
predictionResult = cModel.transform(test_f)
predictionResult.show()   